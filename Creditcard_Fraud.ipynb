{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas_profiling import ProfileReport\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.metrics import roc_auc_score,precision_recall_curve, classification_report, confusion_matrix, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "file_path = 'C:/temp/dataset/data/creditcard.csv'\n",
    "df = pd.read_csv(file_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Explore Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  We see 31 columns and 284907 rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check top 5 rows of dataset \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check bottom 5 rows of dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The time column is elapsed time for transaction, it could be removed as it is not useful for model. Columns V1-V28 are customer personal info and have been transformated via PCA. \n",
    "# We see the difference in scale between the PCA variables and the column amount suggests that data scaling should be done.\n",
    "df = df.drop('Time', axis = 1)\n",
    "scaler = StandardScaler()\n",
    "df[['Amount']] = StandardScaler().fit_transform(df[[ 'Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any missing values. We see no missing values\n",
    "df.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the frequency count on fraud and non-fraud,  we can see the values are highly skewed.There are 492 fraudulent transactions and  284315 non-fraudulent transactions\n",
    "print(df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define function to prepare data\n",
    "\n",
    "def prep_data(df):\n",
    "    X = df.iloc[:, 0:29]\n",
    "    X = np.array(X).astype(np.float)\n",
    "    y = df.iloc[:, 29]\n",
    "    y = np.array(y).astype(np.float)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prep_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a scatter plot of the data and labels\n",
    "def plot_data(X, y):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that fraud and non-fraud are highly imbalanced\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of input variables. We can see that the distribution of most of the PCA components is Gaussian\n",
    "# drop the target variable\n",
    "data = df.drop(['Class'], axis=1)\n",
    "# create a histogram plot of each numeric variable\n",
    "data = data.hist(bins=100)\n",
    "# disable axis labels to avoid the clutter\n",
    "for axis in data.flatten():\n",
    "    axis.set_xticklabels([])\n",
    "    axis.set_yticklabels([])\n",
    "    pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(['Class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit different ML models' classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "model_rf = RandomForestClassifier(n_estimators = 100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "model_knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "model_svc = SVC(probability=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes \n",
    "model_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Gradient Boosting\n",
    "model_xgb = XGBClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a funciton to calculate precision-recall area under curve\n",
    "def pr_auc(y_true, probas_pred):\n",
    "    # calculate precision-recall curve\n",
    "    p, r, _ = precision_recall_curve(y_true, probas_pred)\n",
    "    return auc(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print ROC, classification report, confusion matrix and AUC\n",
    "def get_model_result(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "    print('The ROC_AUC_SCORE is:\\n', roc_auc_score(y_test, probs[:,1]))\n",
    "    print('The Classification report:\\n', classification_report(y_test, predicted))\n",
    "    print('The Confusion matrix:\\n', confusion_matrix(y_true=y_test, y_pred=predicted))\n",
    "    print('The AUC is :\\n', pr_auc(y_test, probs[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_lr)\n",
    "end = time.time()\n",
    "print('The total Execution Time for LR is : \\n',  end - start)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_rf)\n",
    "end = time.time()\n",
    "print('The total Execution Time for RF is : \\n',  end - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_knn)\n",
    "end = time.time()\n",
    "print('The total Execution Time for KNN is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_svc)\n",
    "end = time.time()\n",
    "print('The total Execution Time for SVC is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_nb)\n",
    "end = time.time()\n",
    "print('The total Execution Time for NB is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_xgb)\n",
    "end = time.time()\n",
    "print('The total Execution Time for XGB is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Based on models' performance metrics, we combine top three classifiers-RF, KNN and XGB in the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemable model basesd on soft voting \n",
    "model_ensemble = VotingClassifier(estimators=[('rf', model_rf), ('knn', model_knn), ('xgb', model_xgb)], voting='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics for ensemble model. We can see that it predicts more number of ture postive and less number of false positive with high recall and AUC. \n",
    "# However, XGB is almost competitive with this ensemable model and use less time\n",
    "start = time.time()\n",
    "get_model_result(X_train, y_train, X_test, y_test, model_ensemble)\n",
    "end = time.time()\n",
    "print('The total Execution Time for Ensembled model is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Compare this ensemable model with SMOTE(Synthetic Minority Over-sampling Technique) in XGB on this imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the feature X and y\n",
    "X, y = prep_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMOTE  \n",
    "method = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled data, Now we see the  minority class is now much more prominently visible in the data. \n",
    "plot_data(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value_counts on the original labels y\n",
    "print(pd.value_counts(pd.Series(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value_counts, we see the number of counts are same in class\n",
    "print(pd.value_counts(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deing resampling method\n",
    "resampling = SMOTE(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sampling method on training data \n",
    "X_resampled, y_resampled = method.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeliine to chain SMOTE and model together\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('XGB', model_xgb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pipeline to combine with model\n",
    "pipeline.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probability\n",
    "probs = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance results .\n",
    "start = time.time()\n",
    "print('The ROC_AUC_SCORE is:\\n', roc_auc_score(y_test, probs[:,1]))\n",
    "print('The Classification report:\\n', classification_report(y_test, predicted))\n",
    "print('The Confusion matrix:\\n', confusion_matrix(y_true=y_test, y_pred=predicted))\n",
    "print('The AUC is :\\n', pr_auc(y_test, probs[:,1]))\n",
    "end = time.time()\n",
    "print('The total Execution Time for SMOTE Combined with XGB is : \\n',  end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model predicts more number of TP and FP with almost the same AUC as ensemabled model and XGB alone. So overall, ensemabled model is best if time is not a matter, otherwise, XGB could be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
